[["index.html", "sattag_supplement Preface", " sattag_supplement wrc 2025-02-18 Preface I hope this document will serve as an organized collection for notes on the hardware specifications, operation, programming, errors, and data outputs of Wildlife Computers tags used at DUML. These notes have been compiled from personal experience and experimentation, as well as through conversations with other users and technical support staff at Wildlife Computers. To the best of my ability everything included here is as accurate as possible and I’ve tried to indicate clearly where there are areas of uncertainty. That said, the tags can and do change including in the hardware, firmware, and software and so this information may become out of date without notice. My goal is to provide some memory of what we have figured out. In the course of dealing with these tags, I’ve ended up writing a lot of code, including a couple of shiny apps, a couple of R packages, and various other odds and ends mostly in R. Where helpful, I’ll include or link to code that I think is decent enough to share. Most of the code I’ve written is hosted on my github. Let me know if you are interested in contributing! Finally, thanks to all the folks who provided information on these instruments or engaged in helpful discussions about how they might possibly be working. Including (but not limited to): Russ Andrews, Heather Baer, Robin Baird, Heather Foley, Thomas Gray, Dave Haas, Douglas Nowacek, Nicola Quick, Andrew Read, Matthew Rutishauser, Greg Schorr, Brandon Southall, Jeanne Shearer, Zach Swaim, Daniel Webster, and Kenady Wilson. ~wrc Document last updated: 2025-02-18 "],["todo.html", "1 TODO", " 1 TODO tables are too wide in 5.3 find more serial numbers for goniometers example tag showing messages received by gonio by sat and by both Add more settings to 3.2 including fastloc settings Add words to the glossary as needed code folding? https://stackoverflow.com/questions/45360998/code-folding-in-bookdown "],["the-tags.html", "2 The Tags 2.1 Spot 2.2 SPLASH10 2.3 Fastloc (SPLASH10-F)", " 2 The Tags Wildlife Computers has sort of a confusing naming scheme for their tags and I don’t completely understand. The two key things to know are there are the guts of the tag (circuit board / sensors) and they came come in a variety of shapes, which is just the housing but makes the names confusing. Wildlife Computers folks seem to refer to these as shapes so I’ve used that here. 2.1 Spot Most of the spots were deployed pre-BRS and so I don’t have much to say about them. They are primarily Argos location tags. They are programmed differently (through the tag agent) than the SPLASH10 tags (see below). They are deployed in a similar manner and the data is retrieved similarly. 2.2 SPLASH10 SPLASH10 tags are the majority of the tags that have been deployed during BRS. These tags have the ability to generate some depth and dive profile-like data. Some folks also refer to these as MK10 tags. I think this refers to the circuit board. Most (if not all) of ours have been SPLASH10-292. The 292 refers to the shape, but I don’t know what it means specifically. This shape is designed for remote deployment with the LIMPET (Low Impact Minimally Percutaneous Electronic Transmitter) configuration (Andrews et al. 2008). These tags are programmed with MK10 host. Wildlife Computers claims that this shape is “good” to 2000 meters. I’m not sure exactly what “good” means but they do test these things in a pressure chamber. There do seem to be at least two configurations for the pressure transducer though. standard: -40 to 1000 meters at 0.5 meter resolution. Overpressure rating on the transducer is 2000 meters. Though note that the reading pins1 at around 1700 meters. extended depth: -80 to 2000 meters at 1.0 meter resolution. Overpressure rating on the transducer is 3000 meters. Wildlife Computers technical staff reported that transducers are temperature calibrated to about 1% of full scale though the manufacture spec is tighter. Wildlife Computers has not been willing to give us the manufacturer or any detailed specs on the pressure transducers. Greg Schorr put at least one SPLASH10 tag in a pressure chamber up to 3000 meters and found a max error of +/- 2.5% beyond the Wildlife Computers calibration (Schorr et al. 2014). In addition to these configurations, Wildlife Computers technical staff has indicated that there was a design flaw in mounting pressure sensors until recently (at least 2017?) which is to be corrected (already is corrected?) and will reduce failures (§ 8). 2.3 Fastloc (SPLASH10-F) We’ve deployed a limited number of SPLASH10-F tags, which basically have everything that SPLASH10 have plus the addition of a fastloc board and antenna. I think the shape on these is 333, but I’m not positive. These tags are a little bigger and heavier than the SPLASH10 but are still deployed in a similar fashion using the LIMPET configuration. These tags are also programmed with MK10 host. References Andrews, R. D., R. L. Pitman, and L. T. Ballance. 2008. Satellite tracking reveals distinct movement patterns for Type B and Type C killer whales in the southern Ross Sea, Antarctica. Polar Biology 31:1461–1468. Schorr, G. S., E. A. Falcone, D. J. Moretti, and R. D. Andrews. 2014. First long-term behavioral records from Cuvier’s beaked whales (Ziphius cavirostris) reveal record-breaking dives. PloS one 9:e92633. By pins I mean as the pressure increases the sensor continues to read some arbitrary max value.↩︎ "],["mk10-host.html", "3 MK10 Host 3.1 Recording Settings 3.2 Detailed settings descriptions and comments", " 3 MK10 Host 3.1 Recording Settings It is sometimes nice to have a table of all of the settings for a large number of tags. There is not a great way to do this. I recently asked if Wildlife Computers would be willing to share the specifications on the wch file format so that I could read in the settings automatically. They were not willing to share this. Without a specification, it would be very time consuming to parse the wch files. Instead there is an option in MK10 Host to output an html file which is called a settings report (usually actually with an htm extension). This file is a massive html table with most of the settings and is somewhat easier to parse. Figure 3.1: Partial example of an html settings report for a SPLASH10 tag I’ve written a bit of code to try to parse these automatically and put it in the sattagutils package (§ 6). The relevant function is sattagutils::wch_html2df(dir, file). It takes as parameters a directory path of htm files or a single htm file to convert to a data.frame which it returns. I try to do the minimum processing at this step, though I change commas to semicolons and otherwise remove characters that would be illegal in an R data.frame. The result can be further processed and saved. config_df &lt;- sattagutils::wch_html2df(&quot;examples/configs&quot;) dim(config_df) ## [1] 2 133 # take a peak at some fields knitr::kable(config_df[, c( &#39;wch_filename&#39;, &#39;Tags_Serial_Number&#39;, &#39;PTT&#39;, &#39;Ignore_dives_shallower_than&#39;, &#39;Ignore_dives_shorter_than&#39; )], booktabs = TRUE) wch_filename Tags_Serial_Number PTT Ignore_dives_shallower_than Ignore_dives_shorter_than Example1-53643-15A1024.htm 15A1024 53643 50m 33m Example2-174749-17A0528.htm 17A0528 174749 75m 30s There are some irregularities in the html configuration files including some typos. When these are fixed the wch_html2df function will break. In addition, some settings change their label when turned on or off or disappear completely to be replaced by other options. One note about generating the settings files in general: They can be generated both from a real tag plugged into MK10 Host, but also after the fact from wch files. Importantly, when generated from wch files, they do not retain the calibration information on the various sensors. Figure 3.2: An example of the sensor calibration section of an html settings report generated from a wch file as opposed to from a real tag. If an html report is not made from the physical tag for deployment as far as I know these data are lost (as in the image above). It is possible that Wildlife Computers keeps records of the calibrations and could send them upon request for particular tag serial numbers, but I do not know. 3.2 Detailed settings descriptions and comments 3.2.1 Host Settings MK10Host version This is the windows software that connects to and programs the tag. User Name This is the username of the person who generated the configuration report. This might be the person who programmed the tags or might also be generated after the fact by someone else from the wch file. 3.2.2 Time and Date Settings PC Date UTC Date and time of report generation. Tag Date Date and time on tags clock. PC UTC offset Time difference from PC local time to UTC. 3.2.3 General Settings Tag’s Serial Number This is a identifier of the tag. Not to be confused with the PTT. Password Mostly this is MK10, occasionally something else. If you can’t figure it out for a tag check with the programmer (usually you can figure out who that is from the username). User’s Identifier This is a user inputted name for the tag. E.g., “Zc” would be a tag set up to be deployed on a Ziphius cavirostris. This will not be the DeployID because it is set before tags are deployed and the DeployID isn’t necessarily known beforehand. Argos Ptt number PTT is the Argos platform terminal transmitter (See Argos user manual section 4.1). In our case this is embedded in a tag. The first decimal number is how CLS identifies the tag on their web portal. The second hexadecimal number in parenthesis is what the transmitter actually squawks. This is, for instance, exclusively the number we get from the goniometer (§ 7). These two numbers could be related in some predictable way to each other, but if they are I can’t tell… LUT stands for the local user terminal. LUTs download data from satellites and are how we get anything back at all from Argos. Repetition Intervals This is set to 15s (at-sea) for all tags. This is the interval at which the tag will send another Argos message when dry. Number of Argos transmissions If the report was generated with a tag actually connected during programming this will be the actual cumulative number of Argos transmissions at that time. This is often in the several hundred range, because wildlife computers will run the tag before delivery and the programmer or others may run the tag for brief periods for the purposes of testing. If the report was generated after the fact from a configuration file and not the actual tag this field will read 0. Tagware version Hardware version Battery Configuration A description of the number and type of batteries. This depends on the tag. For example, 2 x M3. Battery Capacity For example, 1500mAh. Battery is not classified as dangerous goods This is shipping / export info. Deploy from Standby on Depth Change This is set to ‘no’ on our tags. Owner It looks like this is usually the wildlife computers address for some reason. Bytes of archive data collected As above, if the real tag is connected this will read how much data has been stored to the onboard memory. If the report is generated form a configuration file this will read 0. Bytes of histogram and profile data collected As above, if the real tag is connected this will read how much data has been stored to the onboard memory. If the report is generated form a configuration file this will read 0. 3.2.4 Data to Archive Settings NOTE: these settings are tricky. Most of them should not matter since we never recover the archive. Status messages pull data directly from the sensors, so should not be dependent on any of these sampling periods. However, Wildlife Computers technical staff has indicated that these settings are used by the tag to determine how to sample for some of our data streams. In particular Depth and wet/dry appear to be important. Apparently there is a hack to get a faster wakeup to the tag by decreasing the sampling period for wet/dry but we have not tested this.2 Depth Internal Temperature External Temperature Depth Sensor Temperature Battery Voltage Wet/Dry Wet/Dry Threshold Most tags are set to Dry if &gt; 150. The conductivity sensor reports 8 bit resolution. This is an empirical value determined by Cascadia and perhaps others to be effective. Wildlife Computers has an dynamic threshold algorithm, but early tests by Cascadia and others found it to be unsatisfactory. It is possible that algorithm may have been updated to function better now, but since this threshold appears to be working adequately we have not changed it. Sampling Mode Set to wet or dry. Automatic Correction of Depth Transducer Drift Set to Using first dry reading on most of our tags. The other option is by most common shallow depth; this is for sharks. This means the pressure transducer is read when the tag becomes dry and can apply a correction if this reading is not 0. Wildlife Computers technical staff indicated that this correction is only applied 1 meter at a time (on 2000m tags) and the correction will only be applied to +/- 40m. 3.2.5 Data to Transmit Settings Histogram Selection All of the histograms have been disabled for the latest BRS tags. Our goal is to get continuous dive record either through the behavior summary data or the series data and so any histogram of dives are not useful. They serve as a reduced representation of the dive record in the case where you wouldn’t expect to get continuous records. The dive histograms can of course be calculated from our behavior records. Since we have not been thinking about or using any kind of temperature data we have also left this off. Histogram Data sampling interval Dive Maximum Depth (m) Dive Duration Time-at-Temperature (C) Time-at-Depth (m) 20-min time-line Hourly % time-line (low resolution) Hourly % time-line (high resolution) Dry/Deep/Neither time-lines PAT-style depth-temperature profiles Deepest-depth-temperature profiles Light-level locations 3.2.6 Histogram Collection Hours of data summarized in each histogram Histograms start at GMT Do not create new Histogram-style messages if a tag is continuously dry throughout a Histogram collection period 3.2.7 Time series messages Generation of time-series messages Time interval between TS samples We selected 5 minute period for our test tags. This was based on average Z. cavirostris dive times for shallow and deep dives while minimizing the number of messages per day. Each time series message contains 48 points, so a higher sampling frequency will result in more messages generated per day. The options are 1.25, 2.5, 5, 7.5, and 10 minute sampling periods. Generating 24, 12, 6, 4, and 3 messages per day respectively. See § 5.2 for details. Channels sampled We have set this to just depth for the BRS test series Z. cavirostris tags. Can also sample temperature. That would double the amount of data transmitted. Start with This is an initial number of days to collect time-series data for. We’ve set this to 14 days for the BRS test series tag configuration. This way it doesn’t matter when the tag turns on from that point onward it’ll record for 14 days and then duty cycle.3 then Duty Cycle with We’ve set this to 0 days off. and We’ve set this to 0 days on. 3.2.8 Dive and Timeline Definition These settings determine what qualifies as a dive. Depth reading to determine start and end of dive We have been typically setting this to wet or dry. There is a risk that if the conductivity sensor is low or fouled or the weather is high that surfacings will be undersampled. See § 8.4 for a more detailed discussion of this issue. While this determines what is a candidate dives it must also pass the following tresholds to be saved as a dive and put in the buffer for transmit. Ignore dives shallower than We’ve set this to be 50 meters for Z. cavirostris and 75 meters for G. macrorhynchus. The maximum is 75 meters. Ignore dives shorter than We’ve set this to be 33 minutes for Z. cavirostris and 30 seconds for G macrorhynchus. (see § 5.1 for details) Depth threshold for timelies 3.2.9 Behavior Messages Generation of behavior messages 3.2.10 Stomach Temperature Messages Generation of stomach temperature messages 3.2.11 Haulout Defintion There is only one field under this heading and the label changes depending on what is set. For cetaceans, you generally do not set this and so it’ll read Tag will never enter haulout state. 3.2.12 Transmission Control Transmit data collected over these last days This is what we call the buffer. This seems simple but it is tricky. The Argos platforms transmit only, they have no idea if a satellite receives anything. In fact they don’t even know if a message is corrupted or not (for example, by water splashing the antenna and interrupting a transmission). The tag only knows whether or not a transmit was attempted. Therefore, the buffer allows some storage of messages so they can be transmitted several times over the course of a few days to increase the likelihood that they will get through the satellite. If the buffer is too short, then messages will be deleted before they have an opportunity to be transmitted successfully. If we have about 9% satellite coverage off the coast of North Carolina, then a message should have to be sent about 10 times for 1 to go through. If the buffer is too long, our probability of new messages being successfully transmitted goes down. In this example, in the beginning of the deployment the queue will be short because the tag hasn’t been attached long enough to generate enough messages. So given a constant number of opportunities (the animal brings the tag to the surface) there will be more attempts per message. As the deployment goes on the message queue will get longer and longer and there will be fewer and fewer attempts for each message, especially impacting the latest messages which entered the queue when there were already a lot of messages. So the key considerations are: data volume (settings dependent), satellite coverage (location dependent), and transmit opportunities (species and settings dependent). Other crucial settings are the transmit budget (§ 3.2.17), transmit duty cycle (§ 3.2.15) and collection duty cycle (§ 3.2.13) Cascadia’s experience has been that for deep diving cetaceans a 2 day buffer works well for collecting behavior data and tends to baa lance the above concerns. For our series tag configurations (see § 5.2), we used a 100 day buffer, the maximum, because we were most concerned with successfully receiving all records and adjusted other settings to increase this probability. 3.2.13 Collection days January - December Which days to collect data on. For BRS, we have set these to all of the days. This is a way to duty cycle the tag off for specific days. 3.2.14 Relative transmit Priorities Histogram, Profiles, Time-lines, Stomach Temperature Fastloc and Light-level Locations Behavior and Time-Series Status Set to every 20 transmissions. I don’t think this can be changed? 3.2.15 When to Transmit Settings Initially transmit for these hours regardless of settings below We’ve set this to 24. This comes from Cascadia. I assume the reason is to aid in determining if the tag is working properly and the deployment is OK. Transmit hours This is based on an analysis of a satellite availability conducted by the tagger. For BRS, we have increased transmit hours to times when there is very poor or no satellite coverage but there is a possibility we will be offshore. We can use the goniometer (§ 7) to track animals as well as capture transmitted messages. 3.2.16 Transmit Days January - December Same as above for collection, but for transmission. 3.2.17 Daily Transmit Allowance January - December Can only give one number per month which will apply to every day in that month. We’ve been using 700-750 for G. macrorhynchus and 470 for Z. cavirostris. In both cases we’ve set to not accumulate and not optimize for battery life. These numbers have been tested as effective by Cascadia. Some things to consider for transmission number include animal activity patterns, satellite coverage, and battery life. Transmissions use battery, so setting this number very high could lead to early battery drain. In the case of Z. cavirostris this is unlikely since they spend little time at the surface they generally do not hit the 470 transmit limit. G. macrorhynchus however can spend hours logging at the surface and may go through their entire transmit budget relatively quickly. Making this number smaller helps avoid the situation where the animals are spending a lot of battery life but not sending very informative data (not moving quickly or diving very much). The drawback to making this number small is that it may be used up before good satellite windows during the day. 3.2.18 Channel Settings These settings pertain to the sensors and their calibrations. 3.2.19 Depth This field also indicates if the tag is extended depth or not by looking in the value of the field under Range and Resolution. Correction factors Errors Compensation factors Errors 3.2.20 Internal Temperature Correction factors Errors 3.2.21 External Temperature Correction factors Errors 3.2.22 Depth Sensor Temperature Correction factors Errors 3.2.23 Battery Voltage 3.2.24 Wet/Dry SPLASH10 tags have a 1/2 second wake up time, while SPOT6 have a 1/4 second wake up. By increasing the sampling rate, we can hack the SPLASH10 to have a 1/4 second wake up at the cost of power. Is this because it is actually logging at that higher sampling rate? I’m not sure.↩︎ It doesn’t seem to ever be exactly 14 days, I don’t know how it counts…↩︎ "],["datastreams.html", "4 Data streams 4.1 Date formats 4.2 Malformed 4.3 Field details 4.4 Fastloc streams on the portal", " 4 Data streams The Wildlife Computers documentation (see § 10, in particular the spreadsheet file descriptions) does a decent job of explaining the data in most of these streams. I’ve collected some notes here that aren’t documented or where the documentation is confusing. In addition, I’ve pointed out where some of the csv files for the data streams are not well formatted and so they may cause problems if you are trying to ingest them into your database. This was the main motivation for writing sattagutils. 4.1 Date formats %m/%d/%Y %H:%M:%S *-All.csv: Loc. date, Msg Date %H:%M:%S %d-%b-%Y *-Behavior.csv: Start, End *-Corrupt.csv: Date, Possible.Timestamp *-SeriesRange.csv: Start, End *-Status.csv: Received, RTC *-Summary.csv4: EarliestXmitTime, LatestXmitTime, EarliestDataTime, LatestDataTime Time (%H:%M:%S) and date (%d-%b-%Y) in separate fields *-FastGPS.csv: Time, Day *-RTC.csv: TagTime, RealTime; TagDate, RealDate *-RawArgos.csv: PassTime, MsgTime; PassDate, MsgDate *-Series.csv: Time, Day 4.2 Malformed There are minor things to watch out for in all of the data streams including inconsistent capitalization schemes and field names with lots of whitespace and other odd characters. A couple streams in particular are saved by default into files with the csv extension, but they aren’t proper csvs. 4.2.1 *-RawArgos.csv This stream always has 4 lines extra lines at the bottom which aren’t part of a csv. 2 blank lines are followed by 2 line (1 header) table with PTT, Diags, Passes, and Msgs. If you’re trying to load this file into R as a csv you’ll get an error. 4.2.2 *-FastGPS.csv This stream has 3 blank extra lines before the header at the beginning. Again you can’t load this file straight into R as a csv without an error.You also often get multiple versions of *-FastGPS.csv in a directory downloaded from the portal (see § 4.4 for details). Instead of DeployID and Ptt this stream uses Name. I believe it defaults to DeployID unless none is present in which case it displays Ptt. This is the general behavior of DeployID as far as I know actually. In addition if decoding with DAP / Argos Message Decoder and there are multiple tags in a PRV file, each new tag will include the 3 blank lines and header in the middle of the csv. 4.2.3 *-Labels.csv There is something wrong with this file. I think it be missing an EOF or something? It is also a tall table instead of a wide table (like every other stream). 4.3 Field details 4.3.1 *-Behavior.csv Start / End These times are actually a little confusing. To understand them, recall that blocks of summary information about ‘dives’ and ‘surface’ behavior events are collated into a single ‘message’ for uplink to Argos satellites. The start and end times are not actually recorded on a clock for any of these behavior events. Instead something like a running count of seconds since the start of the message block (which is a clock time) is uplinked and then everything is converted back to clock time for the csv file output. I think this is a method to save space and make the messages smaller since Argos bandwidth is limited. This would all be fine, however, the start time of the message is recorded without seconds 5. This is a big deal. The start time of a message is only recorded to minute accuracy. All events within a message block are recorded to second accuracy (in number of seconds since the start of message) but the clock times reported in the behavior csv are off by an unknown and variable amount of seconds. This is troublesome if you are looking at coordinated behavior among multiple animals. It can also lead to overlapping times when a new behavior event starts (at the beginning of a message block) before the previous one ends (at the end of the preceding message block) 6. DepthMax / DepthMin These are confusing because they both actually refer to an estimate of the maximum depth on a dive. The tag is actually sampling at some rate specified in the settings (we’ve been using 1 second) and using that data to generate the summary information about dives that ends up in the behavior stream. The sampled value for the maximum depth on a dive is encoded for upload to satellite and in that process some resolution is lost. So \\(DepthMax - DepthMin\\) is the error band for the real maximum depth. Note that this is just error in the sense of resolution of the data encoding, it doesn’t actually have anything to do with the measurement error in the pressure transducer. There is more chitchat about this in § 5.1. DurationMax / DurationMin I do not know how these are generated, but I would like to know. 4.3.2 *-Series.csv DRange the +/- error on the series depth 7. This is confusing because it is called DRange which I assume stands for depth range and the text in the docs is ambiguous 8. So to calculate the actual error band it would be \\(2 \\cdot DRange\\). This is error in the sense of resolution of the data encoding, it doesn’t actually have anything to do with measurement error in the pressure transducer. There is more chitchat about this in § 5.2. 4.3.3 *-Status.csv Note an important difference between the standard and extended depth pressure transducer configurations on SPLASH10 tags. The standard tags have a 0.5 meter resolution and report Zero Depth Offset in counts, so 30 counts is actually 15 meters. This is processed out in the *-Behavior.csv depth fields as well as the Depth field in the *-Status.csv. Confusing? Zero Depth Offset is applied only 1 (count / meter?) at a time and only goes up to +/- 40 (count / meter?). At drifts greater than this, no offset is applied (and so if you were just looking at offset it would look like everything is normal with the transducer (See § 8). Type indicates whether a status message has passed a CRC (cyclic redundancy check; an error checking mechanism). If the value in this column is ‘CRC’ then the message did in fact pass the CRC. If the value is blank then there was no CRC, but the message is not necessarily corrupt. If a message fails CRC it is actually put in the *-Corrupt.csv stream. Probably the most common reason for this is the transmission was interrupted by water splashing on the antenna (or the animal submerging). Note also that messages which pass CRC can still be corrupted in different ways. Between 2016 and 2017 an error was corrected which was causing too few status messages to come through. For example, prior to 2017, number of transmits never came through on a CRC message only on messages with a blank value in the type field. It is still true that the frequency different fields in status are populated varies from field to field. 4.4 Fastloc streams on the portal There are a couple of ways to set up Fastloc position solving on the Wildlife Computers portal. We have generally selected the “manual” option. This means that solved positions are not automatically incorporated into the platform (tag) archive under my data on the portal (with an exception as we’ll see below). The *-FastGPS.csv data stream is updated with the latest ids and ranges, but these are not solved. Figure 4.1: Example of how to download the main data archive for a fastloc (SPLASH10-F) tag on the Wildlife Computers portal. To initiate a solve click on location processing and create new process. The results of which you can see below. If we wanted to set up an automatic location processing schedule we could select on the drop down menu. Figure 4.2: Location processing interface on the Wildlife Computers portal. You can see in this example that we’ve already requested the locations to be solved 19 times. Downloading this run of the solver will produce an archive with three files in it. And you’ll notice a slightly different naming convention which includes the number of the solving run. Figure 4.3: Archive contents from an archive of solved positions downloaded from the location processing interface on the Wildlife Computers portal. These files are not automatically transferred to the general archive on the main screen of my data on the portal except for the very first solve 9. Here is what that archive looks like for this tag: Figure 4.4: Comparison of archive contents of data downloaded from location processing interface (top) and the main my data interface (bottom) on the Wildlife Computers portal. So between the two archives we actually have three copies of *-FastGPS.csv, *-FastLocGPS.kmz, and *-Locations.csv and all have different contents. Care must be taken, therefore, when downloading data from the portal for fastloc tags, especially if you’d like to ingest them into a database. I’ve never seen ReleaseDate and DeployDate so I’m not sure if they are implemented in the same way or not.↩︎ I think they are rounded, but it is a little hard to tell.↩︎ Another case of overlapping message blocks occured from a corrupt message which apparently wasn’t caught by the decoding software. This produced one extremely long message which overlapped with many others in time. We also sent some overlapping G. macrorhynchus behavior records to Wildlife Computers but have not heard back. I am not sure if these were just the result of clock inaccuracy or not.↩︎ I assume TRange is the same but do not know for sure.↩︎ See page 22 of Spreadsheet-File-Descriptions.pdf (§ 10)↩︎ Since we’ve always been using the manual download setting I don’t know if this is generally true or only true for manual solving.↩︎ "],["settings-regimes.html", "5 Settings regimes 5.1 BRS style behavior 5.2 BRS style series only tags 5.3 BRS fastloc tags", " 5 Settings regimes 5.1 BRS style behavior Quick et al. (2019) covers how and why we set the BRS behavior tags for Z. cavirostris. I’ll provide only a very brief summary here. Also I have some chitchat about the depth resolution because it is relevant for comparisons with the series only tags (see § 5.2). 5.1.1 Key settings We turned off all data streams we could besides behavior to limit the number of extraneous messages generated per day. Mainly this consisted of getting rid of histograms. Start and end of dives was defined by the conductivity sensor. For Z. cavirostris, we set a 33 minute, 50 meter dive minimum. This should correspond to approximately the 800 meter dive cut off (foraging dives). For Globicephala macrorhynchus, we set a 30 second, 75 meter dive minimum. This was the maximum dive minimum depth available. 5.1.2 Benefits More complete behavior records (it worked really well!) Longer continuous stretches of behavior 5.1.3 Costs and risks Battery usage? Hard to tell. I’ll look into this more. Reduced dive data resolution (i.e., no shorter duration dives). 5.1.4 Behavior data depth resolution I’ll use the two example tags to peek at the resolution of max depth on the behavior data stream. These are baseline Z. cavirostris tags so they include a variety of depths (not just dives over 33 minutes in duration). Depth range is really the bin width or resolution due to the encoding of the data and not the accuracy of the pressure transducer. Why did Wildlife Computers put an expensive pressure transducer accurate to 1 meter in the tag but always report coarser resolution? One answer is the higher resolution depths are recorded on the archive on board the tag, we just don’t happen to usually get tags back with cetaceans. The other answer is to save bandwidth when transmitting data. Fig. 5.1 sort of makes sense. The resolution becomes coarser as you go deeper and there seems to be a step function at consistent breaks. extags &lt;- sattagutils::batch_load_tags(&quot;examples/tags&quot;) beh &lt;- sattagutils::getstream(extags, &quot;behavior&quot;, squash = TRUE) dives &lt;- beh[beh$What == &#39;Dive&#39;, ] dep &lt;- apply(dives[, c(&#39;DepthMax&#39;, &#39;DepthMin&#39;)], 1, mean) deprange &lt;- (dives$DepthMax - dives$DepthMin)/2 plot(dep, deprange, xlab = &quot;Max Depth (meters)&quot;, ylab = &quot;Depth Range (meters)&quot;, las = 1 ) Figure 5.1: Relationship between max depth (behavior data) and bin resolution in meters. We can also graph this as percent of max depth which might make more sense. You can see in Fig. 5.2 it looks like the depth bins (resolution) were designed to keep the error somewhere between 1-3% of max depth. That’s OK that makes sense. plot(dep, (deprange/dep)*100, xlab = &quot;Max Depth (meters)&quot;, ylab = &quot;Depth Range (% of Max Depth)&quot;, las = 1 ) abline(h = mean(deprange/dep)*100, lty = 2) Figure 5.2: Relationship between max depth and bin resolution expressed as percentage of max depth. Broken line shows mean depth range. 5.2 BRS style series only tags The bandwidth limitations of non-recoverable satellite-linked biologging instruments in combination with the behavior of infrequent surfacers such as Ziphiids produce tradeoffs between data record length (longevity), data detail (resolution), and data gaps (completeness). The relative importance of these three tradeoffs will depend on the research questions being addressed by biologging data in a particular study and an equal maximization function may not always be desirable. For example, in the baseline data collection off Cape Hatteras, SPLASH10 instruments were programmed to prioritize longevity and data resolution. Specifically, multiple data streams were collected at relatively fine sampling rates, and data collection was duty cycled to increase deployment length, necessarily introducing data gaps. During BRS, to ask if MFAS disrupts deep foraging dives, settings were chosen that prioritized long duration and complete records at the cost of resolution, i.e., shallow dives were not recorded (Quick et al. 2019). While the BRS behavior tags were very successful in producing continuous records, there was a desire for higher resolution on Z. cavirostris. We turned to the series data stream as a possible alternative. Typically, series data had been duty cycled on our baseline tags and turned off for BRS tags to save message bandwidth. In this tag configuration, we turned off behavior data collection and transmission and only collected series data 10. Figure 5.3: A conceptual diagram showing the trade offs among data record length (longevity), data detail (resolution), and data gaps (completeness) and the strengths and weaknesses of each of three tag setting regimes for Z. cavirostris: (a) baseline (pre-BRS) (b) BRS behavior tags which recorded only dives over 33 minutes in length, and (c) the series only tag configuration described here. 5.2.1 Benefits A true time series of depth including shallow dives No reliance on conductivity for start and end dive definitions Data can be down sampled to mimic behavior data for comparability with previous deployments Pressure transducer failures may be easier to diagnose 5.2.2 Costs and Risks Tradeoff between sampling rate and risk for gaps. Higher resolution = more messages generated per day. Durations may be less precise than those defined by the conductivity sensor 16 depth bins based on minimum and maximum dive during each message = slightly different depth resolution from message to message and in general less resolution than behavior data (though note that behavior dive events only provide 1 depth per dive). 5.2.3 Settings decisions Based on the distribution of messages per day we get from behavior tags (see Quick et al. (2019)), we settled on a series sampling period of 5 minutes. Each message contains 48 points so this period produces 6 messages per day. To limit gaps in the data we activated data collection for 14 days and then continued transmission while ceasing collection. We set a data buffer to 100 days (the maximum) with the hope that all gaps would back fill before the end of the tags life. Since there is an interaction between animal behavior and satellite availability these figures are species specific as well as field site specific. We made educated guesses based on the performance of the behavior tags (§ 5.1) to come to these figures11. Ultimately, due to the large number of variables, a field test was required to assess the series tag performance. Nevertheless, I’ve included a bit of additional chitchat about sampling period below. 5.2.3.1 Sampling period As mentioned above, we choose a 5 minute sampling period as a trade off between aliasing shorter dives and generating more messages than could reasonable be uplinked. We can take a look at the same dive profile at different sampling period from some of the baselines tags which we sampled at 2.5 minutes. Above 5 minute sampling periods aliasing becomes quite apparent (Fig. 5.4). # this function is currently missing from sattagutils (issue #12) source(&quot;helper_functions/findgaps.R&quot;) s2_5 &lt;- sattagutils::getstream(extags[[1]], &quot;series&quot;, squash = TRUE) # just look at a couple of messages s2_5 &lt;- s2_5[1:(48*4), ] # make downsampled versions for # 5 min, 7.5 min, and 10 min sampling periods s5 &lt;- sattagutils::resample_ser(s2_5, 5) s7_5 &lt;- sattagutils::resample_ser(s2_5, 7.5) s10 &lt;- sattagutils::resample_ser(s2_5, 10) # plot par(mfrow = c(4, 1), mar = c(0.1, 4.1, 0.1, 0.1)) sattagutils::plot_series(s2_5, ylim = c(-1100, 0), las = 1, xaxt = &#39;n&#39;, xlab = &quot;&quot;, ylab = &quot;&quot; ) legend(&quot;bottomright&quot;, legend = &quot;period = 2.5 min; 12 msg / day&quot;, bty = &#39;n&#39;) sattagutils::plot_series(s5, ylim = c(-1100, 0), las = 1, xaxt = &#39;n&#39;, xlab = &quot;&quot;, ylab = &quot;depth (meters&quot; ) legend(&quot;bottomright&quot;, legend = &quot;period = 5 min; 6 msg / day&quot;, bty = &#39;n&#39;) sattagutils::plot_series(s7_5, ylim = c(-1100, 0), las = 1, xaxt = &#39;n&#39;, xlab = &quot;&quot;, ylab = &quot;&quot; ) legend(&quot;bottomright&quot;, legend = &quot;period = 7.5 min; 4 msg / day&quot;, bty = &#39;n&#39;) sattagutils::plot_series(s10, ylim = c(-1100, 0), las = 1, xaxt = &#39;n&#39;, xlab = &quot;&quot;, ylab = &quot;&quot; ) legend(&quot;bottomright&quot;, legend = &quot;period = 10 min; 3 msg / day&quot;, bty = &#39;n&#39;) Figure 5.4: Real series data collected on a baseline Z. cavirostris SPLASH10 tag is shown in the top panel and successively downsampled to represent other possible sampling periods in following panels. 5.2.4 Series data depth resolution We can use the same tags as above (§ 5.1.4) to take a peak at the depth resolution in the series data. In this case, though we aren’t just looking at max depth of a dive but of a real time-series of depths at regular sampling intervals. You can see in Fig. 5.5 that the bin widths are substantially greater than for the behavior data. The odd fan pattern is actually created by the somewhat dynamic assignment of bins (based on min and max in a given message of 48 samples) and is explained further below. ser &lt;- sattagutils::getstream(extags, &quot;series&quot;, squash = TRUE) plot(ser$Depth, ser$DRange*2, xlab = &quot;Depth (meters)&quot;, ylab = &quot;Depth Range (meters)&quot;, las = 1 ) Figure 5.5: Relationship between depth (series data) and bin resolution in meters. When we look at percentages (Fig. 5.6), some are extremely high. Though, note that the very high percentages are all occurring at shallow depths. When the depth range is greater than 100% the mean depth is just 13 meters. Fig. 5.7 shows a zoom for depths of 250 meters and above. The mean depth range is still 21% of reported depth here 12. plot(ser$Depth, (ser$DRange*2 / ser$Depth) * 100, xlab = &quot;Depth (meters)&quot;, ylab = &quot;Depth Range (% of Depth)&quot;, las = 1 ) Figure 5.6: Relationship between depth (series data) and bin resolution expressed as a percentage of depth. plot(ser$Depth[ser$Depth &gt;= 250], ((ser$DRange*2 / ser$Depth) * 100)[ser$Depth &gt;= 250], xlab = &quot;Depth (meters)&quot;, ylab = &quot;Depth Range (% of Depth)&quot;, las = 1 ) Figure 5.7: Relationship between depth (series data) and bin resolution expressed as a percentage of depth. Only depths 250 meters and greater are plotted. To understand how the 16 bins are set it is neccessary to look at one message at a time. Here I’ll just take the first full13 message of one our example tags. You can see below that there is still a sort of step function similar to what we saw in the behavior data in the bin widths but it is more complex here as there is some variation within each ‘step’ 5.8). t1 &lt;- extags[[1]] ser &lt;- sattagutils::getstream(t1, &quot;series&quot;, squash = TRUE) msg &lt;- sattagutils::getstream(t1, &quot;seriesrange&quot;, squash = TRUE) st_firstfull &lt;- msg$Start[2] ser2 &lt;- ser[ser$Date &gt;= st_firstfull] ser_msg1 &lt;- ser2[1:48, ] plot(ser_msg1$Depth, ser_msg1$DRange*2, las = 1, xlab = &quot;Depth (meters)&quot;, ylab = &quot;Depth Range (meters)&quot; ) Figure 5.8: Relationship between depth and bin resolution in series only data. I’ll plot 10 messages together color coded by message on the same graph to see how the overall pattern is created in bin width and depth (Fig. 5.9). cols &lt;- rep(1:10, each = 48) ser_10msg &lt;- ser2[1:length(cols), ] plot(ser_10msg$Depth, ser_10msg$DRange*2, col = cols, las = 1, xlab = &quot;Depth (meters)&quot;, ylab = &quot;Depth Range (meters)&quot; ) Figure 5.9: The first 10 full messages of one of our example series tags showing the relationship between depth and bin width (twice the DRange). Each color indicates a single message. It might be useful to plot these depth errors on a depth profile (Fig. 5.10); we can use the same tag as above. This tag also happened to be recording simultaneous behavior data so we can superimpose the two data records to compare the different types of depth information (Fig. 5.11). Note that the behavior data is displayed as a dive profile but in truth it is only a sort of pseudo-dive profile 14. Depth error appears fairly negligible for shallow dives, but more substantial for the deeper dives. It is worth noting that the series data in general appears to underestimate maximum depth on deep dives when compared to the behavior data stream. I believe this is because short forays (less than the sampling period) can be missed on the series data, but the max depth on the behavior data stream is actually logged from a 1 second (on our tags) sampling period, even though only one value and a range are retained per dive. sattagutils::plot_series(s5, ylim = c(-1100, 0), las = 1, xaxt = &#39;n&#39;, xlab = &quot;&quot;, ylab = &quot;depth (meters)&quot; ) axis(1, at = s5$Date, lab = NA, tcl = 0.3) segments(s5$Date, -s5$Depth - s5$DRange, s5$Date, -s5$Depth + s5$DRange, col = &quot;red&quot;) Figure 5.10: An example series dive profile (5 min sampling period) with depth bin resolution indicated in red. # this function is still being migrated into sattagutils source(&quot;helper_functions/plot_dives.R&quot;) b1 &lt;- sattagutils::getstream(extags[[1]], &quot;behavior&quot;, squash = TRUE) overlap &lt;- b1$Start &gt;= s5$Date[1] &amp; b1$End &lt;= s5$Date[nrow(s5)] plot_dives2(b1[overlap, ], show_gaps = FALSE, legend = FALSE, start_time = sattagutils::num2date(s5$Date[1]), end_time = sattagutils::num2date(s5$Date[nrow(s5)]), pch = NA, col = &quot;lightblue&quot;, lwd = 2 ) axis(1, at = s5$Date, lab = NA, tcl = 0.3) points(s5$Date, -s5$Depth, pch = 16, cex = .5) lines(s5$Date, -s5$Depth) axis(1, at = s5$Date, lab = NA, tcl = 0.3) points(s5$Date, -s5$Depth, pch = 16, cex = .5) lines(s5$Date, -s5$Depth) segments(s5$Date, -s5$Depth - s5$DRange, s5$Date, -s5$Depth + s5$DRange, col = &quot;red&quot; ) Figure 5.11: Series dive profile (black) and behavior pseudo-dive profile (blue) superimposed for a tag that was recording both streams simultaneously. Red verticle segments indicate depth resolution for the series data. 5.2.5 In the field performance I won’t get into all the details here of in the field performance because we’re still working on assessing it, but I will point out a couple of things. In general, the settings performed well despite some apparently unrelated tag failures. Several of the tags produced near perfect records. After 14 days of data collection it tended to take about another 10-14 days to complete transmission of those records, which brought the total tag close to the median SPLASH10 deployment survival time on Z. cavirostris of about 30 days. Fig. 5.12 shows an animation of the message arrival pattern for a well performing tag, which was placed high on the dorsal fin15. Fig. 5.13 shows a similar plot for a poorer performing tag with a lower dorsal fin attachment. It was also noted in the field that this animal tended to surface low. Arrival of messages is slower for this tag and average number of times each message was received was also lower. Nevertheless, the majority of messages were ultimately received by satellite. These messages were also supplemented by those captured in the field using a boat based receiver (not displayed here, see § 7). Figure 5.12: Series message arrival pattern for a well performing tag with good placement. Data download date can be seen advancing above plot. Bars indicate how many times each series data message was received and pink areas indicate data gaps. Figure 5.13: Series message arrival pattern for a low tag placement. Data download date can be seen advancing above plot. Bars indicate how many times each series data message was received and pink areas indicate data gaps. 5.2.6 Future directions Some considerations for future series based tags include: Higher resolution w/ shorter collection interval Different buffer lengths Is depth resolution sufficient? 5.3 BRS fastloc tags What follows are just some very cursory notes on the fastloc tags (SPLASH10-F) used during BRS; we haven’t looked at any of this in great detail. There was one fastloc equipped tag deployed before BRS (see Baird et al. 2017), but it was a sort of prototype and set up very differently from subsequent tags and I won’t focus on it here. During BRS3, 3 fastloc tags were deployed on G. macrorhynchus and 2 fastloc deployed during BRS4, one on a Z. cavirostris and one on a G. macrorhynchus. 5.3.1 BRS3 fastloc tags 5.3.1.1 Key settings 2 day buffer ignore dives shallower than 75 meters, less than 30 seconds high fastloc transmit priority / low behavior transmit priority max 2 fastloc per hour max 48 per day max 4 failed per hour max 96 attempts per day 5.3.1.2 Performance Table 5.1: Behavior data stream gap summary statistics. Gma = Globicephala macrorhynchus species data record length (days) number of data gaps mean gap length (days) summed gap length (days proportion of data record gapped exgpstag3_1 Gma 19 20 0.57 79 0.17 exgpstag3_2 Gma 11 17 3.33 140 0.52 exgpstag3_3 Gma 24 39 2.30 274 0.47 Table 5.2: Location summary statistics including proportion of locations in each of the location quality classes. Gma = Globicephala macrorhynchus, lq = location quality. species n days lq = Z lq = A lq = B lq = 0 lq = 1 lq = 2 lq = 3 total locations locations per day exgpstag3_1 Gma 19 0.026 0.043 0.10 0.28 0.31 0.17 0.074 231 12 exgpstag3_2 Gma 11 0.025 0.089 0.12 0.20 0.25 0.22 0.089 158 14 exgpstag3_3 Gma 24 0.052 0.052 0.12 0.22 0.28 0.20 0.079 290 12 Table 5.3: Fastloc summary statistics. Gma = Globicephala macrorhynchus species n days n fastlocs n fastlocs per day mean number of sats tracked mean time btwn fastlocs (hours) exgpstag3_1 Gma 19 443 23 4.6 1.03 exgpstag3_2 Gma 11 273 24 5.2 0.99 exgpstag3_3 Gma 24 287 12 4.4 2.14 5.3.2 BRS4 fastloc tags 5.3.2.1 Key settings no behavior data otherwise similar to BRS3 5.3.2.2 Performance Table 5.4: Location summary statistics including proportion of locations in each of the location quality classes. Gma = Globicephala macrorhynchus, Zca = Ziphius cavirostris, lq = location quality. species n days lq = Z lq = A lq = B lq = 0 lq = 1 lq = 2 lq = 3 total locations locations per day exgpstag4_1 Gma 21 0.029 0.098 0.13 0.27 0.253 0.159 0.069 245 11 exgpstag4_2 Zca 24 0.242 0.206 0.12 0.24 0.055 0.018 0.115 165 7 Table 5.5: Fastloc summary statistics. Gma = Globicephala macrorhynchus species n days n fastlocs n fastlocs per day mean number of sats tracked mean time btwn fastlocs (hours) exgpstag4_1 Gma 21 333 15.5 4.7 1.4 exgpstag4_2 Zca 24 182 7.7 5.0 3.1 References Baird, R. W., D. L. Webster, Z. T. Swaim, H. J. Foley, D. B. Anderson, and A. J. Read. 2017. Spatial Use by Odontocetes Satellite Tagged off Cape Hatteras, North Carolina in 2016. Prepared for U.S. Fleet Forces Command. Submitted to Naval Facilities Engineering Command Atlantic, Norfolk Virginia, under Contract No. N62470-15-D8006, Task Order 28, issued to HDR Inc., Virginia Beach, Virginia. August 2017. Quick, N. J., W. R. Cioffi, J. Shearer, and A. J. Read. 2019. Mind the gap: Optimizing satellite tag settings for time series analysis of foraging dives in Cuvier’s beaked whales (Ziphius cavirostris). Animal Biotelemetry 7:5. I’m currently preparing a manuscript describing this setting configuration in detail and comparing series derived dive metrics to those derived from other data collection regimes. Whitepaper: https://osf.io/k8p94/↩︎ Wildlife Computers does have a simulator to test tag configurations. It requires some seed data, but then will run the tag as configured through many simulated deployments and produce sample outputs which can be assessed. We did not avail ourselves of this service in this case.↩︎ Some preliminary analysis of tags which were simultaneously collecting both behavior and series data suggests that on average the series data actually performs better than these theoretical ranges when maximum depths are compared between series dive profiles and behavior dive events.↩︎ Series messages are actually scheduled based on real time depending on the sampling period. That is at 5.0 minute period, messages always end at 00, 04, 08, 16, and 20hrs. Therefore, whenever the tag is deployed it typically sends a shorter first message, before ending on schedule and then being aligned with 48 sample messages from that point forward.↩︎ The shape of the dives here are just to indicate qualitative differences between square, U-, and V-shaped dives as reported in the behavior data stream.↩︎ Dorsal fin placement seems to be a major determinant of how many messages are successfully transmitted to satellite (see Quick et al. 2019)↩︎ "],["sattagutils.html", "6 Sattagutils 6.1 Central concepts 6.2 Quick guide", " 6 Sattagutils I’ve put together some of the functions I use most often into an R package. It isn’t on CRAN but you can get it from github. Perhaps the most useful functionality of sattagutils is that it’ll load a single tag or a directory of tags into R automatically dealing with all of the irregularities and inconsistencies in file and date formats. I’ve also included a handful of plotting and other tools that I tend to use a lot when I’m browsing tags. Most of the functions are especially geared toward either SPOT or SPLASH10 tags, but could conceivable be extended for other purposes. Skip to the Quick guide if you don’t care about the guts of the package. 6.1 Central concepts 6.1.1 S4 Tag data streams are essentially tables with some metadata attached to them. I thought of tags as lists of data.frames with some additional metadata attached to them. In the end I settled on S4 classes to represent tag and data objects. I know some people hate them, but in the end it seemed like the most sensible thing to do. I couldn’t get the seamless functionality that I wanted out of S3. Similarly, the other OO systems in R would’ve gotten in the way, I think. For most operations I wanted tables to behave just like tables. 6.1.2 es4dataframe All of the various sattag data streams inherit es4dataframe: setClass(&quot;es4dataframe&quot;, contains = &quot;data.frame&quot; ) and a constructor: es4dataframe &lt;- function(..., stringsAsFactors = FALSE) { data &lt;- data.frame(..., stringsAsFactors = stringsAsFactors) new(&quot;es4dataframe&quot;, data) } I implemented only the most basic functionality that I needed including as.data.frame, is.es4dataframe, $, [, [&lt;-, and merge although I kind of regret that last one. Most are fairly simple and just pass the buck to the underlying data.frame. For example: setMethod(&quot;[&lt;-&quot;, &quot;es4dataframe&quot;, function(x, i, j, ..., value) { dfin &lt;- as.data.frame(x) dfout &lt;- getS3method(&quot;[&lt;-&quot;, &quot;data.frame&quot;)(dfin, i, j, ..., value) x@.Data &lt;- dfout x@names &lt;- names(dfout) x@row.names &lt;- rownames(dfout) return(x) }) 6.1.3 sattagstream A satagstream a es4dataframe with a couple of extra slots for the type of stream and the original filename that was loaded for the stream. There are various subclass options for sattagstreams and mostly you don’t need to worry about it, the primary uses are importing correctly from the badly formated Wildlife Computer csvs and guessing date formats correctly. 6.1.4 sattag and tagstack A sattag is a list with slots for instrument, DeployID, Ptt, species, location, start, end times, directory, and load date. And finally a tagstack is a list of sattag objects with an extra slot for the directory the stack was loaded from. This is a little overkill probably it could have been just a regular list, but I was committed to keeping the metadata with the data at this point. sattag has its own constructor and a variety of get and set methods for the slots. [ also had to be defined explicitly to get things working as you’d expect (that is [ should return a sattag and [[ should return a sattagstream), but other than that lists behave a little better than data.frames do. tagstack also has a constructor and [ as well as some get and set methods. Everything has nice show methods so you don’t blow up your console peeking at tags. 6.2 Quick guide Install: devtools::install_github(&quot;williamcioffi/sattagutils&quot;) Now we can load a couple of tags into a tagstack and take a peek. extags &lt;- sattagutils::batch_load_tags(&quot;examples/tags/&quot;) extags ## tagstack of 2 tags ## ----- ## 01 - 102465 - ExampleTag001 - 14 streams ## 02 - 77246 - ExampleTag002 - 14 streams So we’ve loaded 2 tags and we can see they both have 14 streams that were imported. We can look inside as if this was any other kind of list. Though, for many analyses we’d probably just want to start pulling out streams of interest from all the tags at this point. tag1 &lt;- extags[[1]] tag1 ## sattag type: Mk10-A ## species: Cuvier&#39;s ## deploy id: ExampleTag001 ## ptt: 102465 ## start data date: 2014-05-13 ## end data date: 2014-07-12 ## ------ ## streams: ## 01 - 102465-All.csv ## 02 - 102465-Argos.csv ## 03 - 102465-Behavior.csv ## 04 - 102465-Corrupt.csv ## 05 - 102465-Histos.csv ## 06 - 102465-Labels.csv ## 07 - 102465-Locations.csv ## 08 - 102465-MinMaxDepth.csv ## 09 - 102465-RawArgos.csv ## 10 - 102465-RTC.csv ## 11 - 102465-Series.csv ## 12 - 102465-SeriesRange.csv ## 13 - 102465-Status.csv ## 14 - 102465-Summary.csv ## ------ ## loaded from: examples/tags//102465 ## loaded on: 2025-02-18 22:23:01.56985 Here you can see a listing of all the streams in a particular tag, and we can take a closer look at the behavior table. beh &lt;- tag1[[3]] beh[, c(&#39;What&#39;, &#39;Start&#39;, &#39;End&#39;, &#39;DepthMax&#39;)] ## What Start End DepthMax ## 1 Message 1399992480 1400001640 NA ## 2 Dive 1399992480 1399993560 387 ## 3 Surface 1399993560 1399993650 NA ## 4 Dive 1399993650 1399994288 347 ## 5 Surface 1399994288 1399994460 NA ## 6 Dive 1399994460 1399998942 1087 ## ------ ## stream type: behavior ## filename: 102465-Behavior.csv All of the times have been converted to datenums for calculations, but can be converted back for human readability with sattagutils::num2date. "],["goniometer.html", "7 Goniometer 7.1 Good goniometers and bad goniometers 7.2 How to test a receiver 7.3 Incorrect dates in the goniometer log 7.4 Decoding Goniometer received messages 7.5 monitorgonio", " 7 Goniometer The Woods Hole Group (formerly CLS America) rents and sells an antenna and receiver that they call the Argos Goniometer. This machine can pick up and directionalize Argos PTTs and even downloads messages. We set up our Goniometer to run in hyperterminal mode connected to a laptop on the boat. Basically this spits the Goniometer log out to a text file. There are a couple of different types of text that are displayed in that log but it is mostly comma separated values. From the R/V Barber and an 3.4 meter mast for the antenna, we have perhaps a range of 4 miles at maximum. Occasionally there are odd atmospheric days that greatly increase range. From Carolina boats (charter boats) like the F/V Kahuna or F/V Hog Wild we get more like 7 miles range. Accuracy of direction readings increases with proximity. 7.1 Good goniometers and bad goniometers The goniometer comes in two main parts. The antenna and the receiver. We’ve never had an obvious problem with the performance of an antenna, and Wood’s Hole Group technical staff suggests that problems generally arise from the receiver. Below is a table of some of the antennas and receivers we’ve tested. All the serial numbers we’ve had have been in the form XXXX-3460313. S/N Instrument Function Condition Notes Owner 0062 Receiver Good Fair GPS antenna plastic cracked16 DUML 0059 Antenna Good Good DUML 0108 Receiver Good Good Rental 0123 Receiver Good Good Rental 0060 Receiver Bad Good poor signal / unreliable dir Rental Demo Receiver Incomplete Good no direction Rental ???? Receiver Bad Good (BRS1) poor signal? / unreliable dir Rental 0135 Antenna Good Fair crossthreaded Rental 7.2 How to test a receiver The simplest test of a reciever is to a turn on a tag briefly. This is best done outside and away from RF interference, but I’ve also just done it in the house and it has worked fine. When the tag is within a few feet of the goniometer the signal strength should be 0 (the strongest possible). Directionality can also be checked in this way by moving the tag in relation to the antenna noting the position of the nubbin which indicates 0° (forward). A better test is with the antenna rigged onto the boat to walk around the boat the with a test tag transmitting to confirm signal strength and directionality. Again, you should be able to get the signal strength to 0 with this method. We’ve also tried leaving the test tag tansmitting on the dock and taking the small boat with a rigged antenna farther and farther away to do a simple range test. This can be tricky, because there are a lot of obstructions and interference near to shore. A better test could be performed at sea with two boats. It is also helpful to informally cross reference wich tags are being received between boats when there are multiple boats rigged with goniometers in the same general area tracking animals. 7.3 Incorrect dates in the goniometer log The goniometer will occasionally produce incorrect dates. It seems like this can occur sometimes when the battery of the receiver is run down very low. This has happened to me especially when there are long periods between field efforts. My best guess is that for some reason the date gets set incorrectly prior to GPS connection in some circumstances. I’ve found the following procedure can fix this problem: Back up current data just in case there is useful information or the date can be repaired. Make sure the unit is well charged and has a good GPS connection. Clear the memory and reset the unit. Turn everything back on and let the unit get a good GPS connection. Turn on a test tag to make sure date, time, and data all look good. If problem persists try repeating the process. 7.4 Decoding Goniometer received messages I’ve written some code to decode the Goniometer received messages. Really I didn’t though, you still need Argos Message Decoder.exe / DAP Processor to decode the messages, but my code transforms the Goniometer type output into something that DAP Processor can read. Eventually this code will be wrapped into the sattagutils package, but for now it lives in a separate github repo. Included in the repo is a full working example. Goniometer log output usually looks something like this: 2017-05-02 14:42:07 : USB Connection to RXG134 2017-05-10 21:00:38 Received : $NPRF,7,17,5,10,21,0,24,0A1FBD4,401677726,7,109,115,-128,-128,-4486401,2133099,10,192,D4049D095018019931F7F4B00960508485440003B6567FC0*46 2017-05-10 21:00:58 Received : $NPRF,7,17,5,10,21,0,44,0A1FBD4,401677737,7,117,113,-127,-127,-4486412,2133177,2,192,D4040555502CF64C3207F4A009605084854000027FC00000*01 The $NPRF indicates that a platform has been favorited, but the log will also display messages from non favorited platforms with the label $NPR. The date, PTT, signal strength, bearing, among other things are recorded here. The last field is the data message. You can use parsegonio() to create a simulated prv file that DAP processor can read: gfile &lt;- &quot;gonio_ex_log.txt&quot; pttkey_file &lt;- &quot;pttkey.csv&quot; source(&quot;parsegonio.r&quot;) output &lt;- parsegonio(gfile, pttkey_file) cat(output, file = &quot;gonio_output.prv&quot;) parsegonio() takes two parameters: pttkey_file is a csv which includes the PTT, hex, and DeployID of platforms of interest (only the PTT and hex are necessary) and gfile is an example Goniometer log. I’ve saved the output to gonio_output.prv here. It seems like there is some flexibility in the prv format and I’ve taken some liberties, mainly because I don’t entirely understand every part of the format. In my simulated prv, for each platform, all the messages from the Goniometer log are lumped under one satellite pass even if they occurred over many days. This isn’t realistic, but DAP Processor doesn’t seem to mind and satellite passes don’t mean anything in this use case anyway. In addition, real prv files include the solved Argos position. Obviously I don’t have one, so I just added a point right outside Pilkey. Once you have the simulated prv output you can just drag it into DAP processor. It helps to have preloaded a workspace with wch files as well as DAP will use the tag settings during decoding, though you can still get something even without them. Figure 7.1: The example goniometer messages included in parsegonio decoded by Argos Message Decoder.exe / DAP Processor csv files can now be exported. Aome of these for instance, *-Locations.csv, *-Argos.csv will be complete junk. Others will mostly make sense, but be sure to ignore anything to do with Dopler locations or satellite passes. 7.4.1 Locations and FastGPS While the Goniometer received messages do not provide Doppler positions there are two ways in which they may assist with real locations. The first way we can more positional information, from Rob Schick, is to use Goniometer bearing to constrain the error ellipses on Doppler positions. See Rob’s figure here on the osf. The second way we can get additional positions is with the FastGPS messages. These messages consist of time and range information for GPS satellites, which are later solved (serverside) for positions. We can use DAP / Argos Message Decoder to solve these positions after the fact even from Goniometer received FastGPS messages. If some of these messages were never received by the satellite then they represent added location data. Care must be taken, however, when thinking about the init locations. DAP / Argos Message Decoder decides which recent location is the best to use as an init and if there is nothing better recently it will use a Argos Doppler position, even a low quality one. Since I’ve simulated the prv file with a fake Doppler position, the FastGPS solver will sometimes use this to seed locations. I have not investigated in detail the consequences of this and have not assessed the accuracy of these positions, but it is on the list. I can think of a couple of solutions to make these FastGPS positions more accurate if indeed these erroneous init positions contribute a lot of positional error: Use the study site centroid as the fake Argos Doppler position in parsegonio. Use the boat position as the fake Argos Doppler position in parsegonio. Since the range of the goniometer is limited, the boat should always be within 10 miles or so of the tag when it receives a message. Use the tag on position as the fake Argos Doppler position in parsegonio. See if I can set the Argos Doppler position to nothing in parsegonio (I think I can) and then combine the gonio prv with the real prv and re-decode them. DAP / Argos Message Decoder then should use a reasonable init location from whatever was received on the satellite nearest in time. 1 is easiest. Then probably 3, then 2 and 4 are tied for being the most complicated since they pull in the most data from other sources. Probably at the very least should let the user set what they want the fake Argos Doppler position / positions to be in parsegonio. I’ll open an issue on the github repo. 7.4.2 Reintegrating Goniometer downloaded messages I haven’t dealt with this much yet. Obviously care should be taken when integrating Argos uplinked/downloaded messages and those from the Goniometer to avoid location, message block, or time confusion. Nevertheless goniometer downloaded data can provide additional dive data, additional fastloc positions (which DAP Processor can solve), as well as additional status messages to aid in error detection. To illustrate the possible complexities of this take a look at a draft BRS analysis flow chart showing some of the processing steps for integrating data between different sources (Fig. 7.2). You’ll see that we sometimes run two Goniometers at once. This step is also tricky because I’m not sure it can be automated. Is there a command line interface for DAP Processor? Or can we upload the simulated prv files to the Wildlife Computers portal using the API? Figure 7.2: BRS analysis (draft) flow chart for sattag processing 7.5 monitorgonio This is a quick hack to display Goniometer output in a user friendly display using shiny. The Goniometer’s screen is tiny, but as mentioned above the hyperterminal mode when connected to a laptop outputs all data to a simple text log file. The shiny app just eavesdrops on this log and uses a key file you load in (csv) to match PTT (hex codes) and display only your platforms of interest. In addition, the program displays the bearing visually on a compass face (circle) and in a table, which is easier to read than the stock software. Figure 7.3: Monitorgonio display tab showing simulated behavior in the field. 7.5.1 Quick guide I’ve actually managed to get this in a package. It isn’t on cran yet though so you’ll have to use devtools to install from github. The dependencies are shiny, shinyFiles, and plotrix. devtools::install_github(&quot;williamcioffi/monitorgonio&quot;) You can run monitorgonio from an interactive r session: monitorgonio::run_monitorgonio() Or you can automatically generate monitorgonio.bat which will run the shiny app for you. I hope with the right paths. Be careful for some reason on windows ‘~’ is interpreted as documents or the onedrive… monitorgonio::make_bat_file(&quot;monitorgonio.bat&quot;) Figure 7.4: Monitorgonio’s data loading screen and instructions. You’ll also have to create a ptt key file (CSV) so monitor gonio knows what to listen for. You can get an example template which comes as a dataframe in the package and save it for editing in your favorite editor. # a template pttkey comes with the package data(pttkey, package = &quot;monitorgonio&quot;) pttkey ## PTT HEX DEPLOYID ## 1 0A1FBF2 test1 ## 2 D7914E1 test2 You can also construct one easily in R and save it as a csv. pttkey &lt;- data.frame( PTT = c(&quot;111111&quot;, &quot;222222&quot;), HEX = c(&quot;0A1FB2&quot;, &quot;D7914E1&quot;), DEPLOYID = c(&quot;test1&quot;, &quot;test2&quot;) ) # save to file write.table(pttkey, file = &quot;pttkey.csv&quot;, sep = &quot;,&quot;, row.names = FALSE) Note: you don’t really need the PTT column which is for the decimal PTT, unless you just want to keep track of it on the screen. What the Goniometer actually receives is the hex. 7.5.2 Testing You can test monitorgonio a bit even if you don’t have a Goniometer connected or a platform handy. To do this you’ll need two instances of R open. Either start monitor gonio with the .bat script, or start it with run_monitorgonio() and then open a new instance of R. In this new instance of R first you’ll need to save the pttkey from above. Save it anywhere you like just remember the path. Next we’ll need a simulated log file. We’ll use a function in a moment to append to the log file as if hits were coming in one by one on the Goniometer, but for now you can just create an empty file: cat(&quot;&quot;, file = &quot;testlog&quot;) Now make sure monitorgonio is running and go to the shiny window and select both the log file and the pttkey using the buttons and navigating to where you saved them. Finally, run the test with: monitorgonio::simulate_gonio(&quot;testlog&quot;) …and you should see hits appearing in the monitorgonio shiny window. 7.5.3 One word of caution about ptts and csvs If you are opening these csv files in excel and if your hex has an E somewhere in the middle and all the other digits are numbers (not letters) then excel will interpret it as scientific notation. For example 12345E2 will be converted into \\(12345 \\cdot 10^2\\) by excel. This is quite annoying and will happen every time you open the file. It’ll look something like this: Figure 7.5: The perils of excel and hex. The best solution is don’t use excel because it is a terrible csv editor. But many folks are most comfortable with editing csvs in excel so in the past I’ve added a notes column that starts with text so the real hex can be recovered if someone accidentally edits it in excel and saves the result. Figure 7.6: Put a notes column and copy and paste hexes back in that might get ruined by excel. Woods Hole Group technical staff suggests function should be unaffected↩︎ "],["failures.html", "8 Failure modes 8.1 Uncontrolled pressure transducer drift 8.2 Pressure transducer drift with recovery 8.3 Unexpected resetting 8.4 Conductivity sensor undersampling", " 8 Failure modes 8.1 Uncontrolled pressure transducer drift Most folks say that when pressure transducers fail they fail completely. For some reason in the SPLASH10 tags uncontrolled drift is far more common. 8.1.1 Diagnosis If there are sufficient status messages, uncontrolled drift should be visible in the Depth field (see Fig. 8.1). Various cutoffs have been suggested. One proposed cutoff is if more than one Depth value exceeds +/- 10 meters (Baird et al. 2018). Wildlife Computers technical staff felt this was arbitrary and since the tags were calibrated to 1% of full scale we should use this as a cut off. Another possible way to diagnose pressure transducer drift in the absence of a good record of status messages would be to identify very unlikely dives based on average vertical velocity (see Fig. 8.2). Baird et al. (2018) suggests 2 m/s as a threshold to flag a dive as potentially unlikely based on time depth data (Baird et al. 2008). See also Tyack et al. (2006) for vertical velocities derived from DTAG data. Once flagged a decision rule would still be needed to determine what constituted a failure (e.g., Shearer et al. 2019)17. 8.1.2 Examples Note that in Fig. 8.1 that while the Zero Depth Offset attempts correction for a period of time, it then drops back once the drift has surpassed about 35 meters or so. In addition, the measurement of the drift also appears to pin at about 80 meters. load(&quot;examples/ex_fail_tags_status.RData&quot;) sta1 &lt;- sta[sta$DeployID == &quot;exfail_2&quot;, ] days_since_deploy &lt;- (sta1$Received - sta1$Received[1]) / 60 / 60 /24 par(mfrow = c(2, 1), mar = c(0, 4.1, 0, 0), oma = c(4.1, 0, 0, 0)) plot(days_since_deploy, sta1$ZeroDepthOffset, las = 1, xlab = &quot;&quot;, ylab = &quot;&quot;, xaxt = &#39;n&#39; ) legend(&quot;topleft&quot;, legend = &quot;Zero Depth Offset (meters)&quot;, bty = &#39;n&#39;) plot(days_since_deploy, sta1$Depth, las = 1, xlab = &quot;days since deployment&quot;, ylab = &quot;&quot; ) legend(&quot;bottomleft&quot;, legend = &quot;Depth at 0 (meters)&quot;, bty = &#39;n&#39;) Figure 8.1: Excessive pressure transducer drift on a SPLASH10 tag commences at approximately 25 days after deployment. In Fig. 8.2, you can see biologically impossible vertical velocities would be necessary to complete the recorded dives. It is not immediately clear if this is indeed due to excessive pressure drift or the more typical type of failure where a pressure sensor might pin to a certain value or rapidly read 0 and full scale in succession. It is also possible that both failure modes are present in this data. Nevertheless, a failure is apparent here, and could be systematical screened for if a vertical velocity threshold were selected. load(&quot;examples/ex_vrate_tag_beh.Rdata&quot;) dur &lt;- (beh$DurationMax + beh$DurationMin) / 2 dep &lt;- (beh$DepthMax + beh$DepthMax) / 2 vrate &lt;- dep * 2 / dur days_since_deploy &lt;- (beh$End - beh$End[1]) / 60 / 60 / 24 plot(days_since_deploy, vrate, las = 1, xlab = &quot;days since deployment&quot;, ylab = &quot;vertical velocity (m/s)&quot; ) Figure 8.2: An extreme example of unrealistic vertical velocities on a SPLASH10 tag suggesting pressure transducer failure 8.1.3 Remedy One proposed remedy when using status messages is to truncate dive data from a tag at the last good status message before a drift threshold was crossed. Note that in some cases dive durations may still be OK if start and end of dives are defined by the conductivity sensor. In these cases as long as drift does not interfere with any of dive qualification requirements (e.g., minimum depth), durations should be accurate even when depths are not. 8.2 Pressure transducer drift with recovery This is a bizarre failure. Wildlife Computers technical staff was unable to provide a possible explanation. 8.2.1 Example sta2 &lt;- sta[sta$DeployID == &quot;exfail_1&quot;, ] days_since_deploy &lt;- (sta2$Received - sta2$Received[1]) / 60 / 60 / 24 par(mfrow = c(2, 1), mar = c(0, 4.1, 0, 0), oma = c(4.1, 0, 1.1, 0)) # divide by two to convert from counts --&gt; meters plot(days_since_deploy, sta2$ZeroDepthOffset/2, las = 1, xlab = &quot;&quot;, ylab = &quot;&quot;, xaxt = &#39;n&#39; ) legend(&quot;topright&quot;, legend = &quot;Zero Depth Offset (meters)&quot;, bty = &#39;n&#39;) plot(days_since_deploy, sta2$Depth, las = 1, xlab = &quot;days since deployment&quot;, ylab = &quot;&quot; ) legend(&quot;bottomright&quot;, legend = &quot;Depth at 0 (meters)&quot;, bty = &#39;n&#39;) Figure 8.3: Pressure transduer drift on a SPLASH10 tag followed by apparent recovery. 8.3 Unexpected resetting Another bizarre failure. Difficult to diagnose (see below) without recovering the tag. Apparently the tag shuts off and turns back on. This appears to clear any messages in the buffer waiting to be transmitted, but the initial configuration and settings are retained. If there is a duty cycle that is initially on for a given number of days, this count appears to be restarted as well. 8.3.1 Diagnosis Wildlife Computers technical staff suggested one of our tags might be resetting based on a pattern of alternating high and low depth readings in the series data stream (Fig. 8.4). In addition, I noticed the following (see Fig. 8.5): There were many more gaps in the data stream than for tags deployed at the same time, even though the deployment placement was good. The tag was apparently ignoring its duty cycle. After a gap in the data, new data previous to the gap never arrived at the satellites (suggesting that the buffer had been lost somehow). 8.3.2 Examples load(&quot;examples/ex_reset_tag_ser.RData&quot;) plot_series(ser, xaxt = &#39;n&#39;, las = 1, ylab = &quot;Depth (m)&quot;, xlab = &quot;&quot; ) axis(1, at = ser$Date, lab = NA, tcl = .3) Figure 8.4: Series data stream from a SPLASH10 during an apparent resetting event. Sampling period is 5 minutes (tick markets along x-axis) Figure 8.5: Series message arrival pattern for an apparently resetting SPLASH10 tag. Data downloaded can can be seen advancing above plot. Bars indicate how many times each series data message was received. Pink areas indicate gaps. 8.4 Conductivity sensor undersampling When set to define dives, if a conductivity sensor is low on an animal or fouled it might not get dry during every surfacing especially during heavy weather. This could lead to an underestimate of numbers of surfacings. Under the BRS behavior tag setting regime (see § 5.1) short surfacings and shallow dives are not being captured anyway so this might be less of an issue. There is a possibility that such undersampling could lead to multiple dives being concatenated if the conductivity sensor never read dry between. 8.4.1 Diagnosis Very hard to diagnose. 8.4.2 Remedy One possible remedy would be to use pressure as the start and end definition of a dive for behavior data. Schorr et al. (2014) compared these two methods of dive definition and found that the conductivity sensor definition was more accurate for determining ventilation periods, however if this is not a priority then a dive definition may help reduce the possibility of undersampling. One draw back to using pressure as a dive definition is that all dive data must be considered unreliable if there is a pressure transducer failure, where when conductivity is used as a dive definition is it possible for durations to be accurate even when depths are not. Series data does not rely on a dive definition at all and so is not subject to this type of failure. References Baird, R. W., D. L. Webster, G. S. Schorr, D. J. McSweeney, and J. Barlow. 2008. Diel variation in beaked whale diving behavior. Marine Mammal Science 24:630–642. Baird, R. W., D. L. Webster, Z. T. Swaim, H. J. Foley, D. B. Anderson, and A. J. Read. 2018. Spatial Use by Cuvier’s Beaked Whales and Short-finned Pilot Whales Satellite Tagged off Cape Hatteras, North Carolina in 2017. Prepared for U.S. Fleet Forces Command. Submitted to Naval Facilities Engineering Command Atlantic, Norfolk, Virginia, under Contract No. N62470-15-D-8006, Task Order 50, Issued to HDR, Inc., Virginia Beach, Virginia. March 2018. Schorr, G. S., E. A. Falcone, D. J. Moretti, and R. D. Andrews. 2014. First long-term behavioral records from Cuvier’s beaked whales (Ziphius cavirostris) reveal record-breaking dives. PloS one 9:e92633. Shearer, J. M. et al. 2019. Diving behaviour of cuvier’s beaked whales (Ziphius cavirostris) off Cape Hatteras, North Carolina. Royal Society Open Science 6:181728. Tyack, P. L., M. Johnson, N. A. Soto, A. Sturlese, and P. T. Madsen. 2006. Extreme diving of beaked whales. Journal of Experimental Biology 209:4238–4253. https://osf.io/3fe5k/↩︎ "],["doppler-positions.html", "9 Doppler Positions 9.1 The “X” 9.2 Error ellipses", " 9 Doppler Positions A lot of ink has been spilled on the accuracy of Argos doppler positions. I won’t try to summarize that literature here. Instead, I’ll include a couple of plots to demonstrate the particular qualities of the data for Z. cavirostris and G. macrorhynchus off Cape Hatteras and some practical advice for filtering. 9.1 The “X” library(colorspace) library(leaflet) # load data load(&quot;examples/ex_ziphius_argos_pos.RData&quot;) uqual &lt;- unique(zloc$Quality) colors &lt;- colorspace::rainbow_hcl(length(uqual)) cols &lt;- 1:nrow(zloc)*NA for(i in 1:length(uqual)) { dese &lt;- zloc$Quality == uqual[i] cols[dese] &lt;- colors[i] } # make a plot m &lt;- leaflet() m &lt;- addTiles(m, urlTemplate=&quot;http://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}&quot;) m &lt;- addCircleMarkers(m, lng = zloc$Longitude, lat = zloc$Latitude, popup = paste(&quot;lq =&quot;, zloc$Quality), radius = 6, stroke = FALSE, fillOpacity = 0.75, color = cols ) m &lt;- addLegend(m, &quot;bottomleft&quot;, colors = colors, labels = uqual, opacity = 1) m Figure 9.1: 2017 Z. cavirostris unfiltered Argos positions. 9.2 Error ellipses Argos satellite are in polar orbits and therefore error tends to be higher in the east-west direction than the north-south direction. par(mfrow = c(2, 2)) xhist &lt;- hist(zloc$Error.Ellipse.orientation, nclass = 50, plot = FALSE) barplot( xhist$counts, axes = TRUE, horiz = TRUE, space = 0, xlab = &quot;count&quot;, ylab = &quot;error elipse orientation (degrees)&quot;, col = &quot;grey75&quot; ) axis(2, at = c(0, length(xhist$counts)/4, length(xhist$counts)/2, 3*(length(xhist$counts)/4), length(xhist$counts)), lab = c(0, 45, 90, 135, 180), las = 1) hist(log(zloc$Error.Semi.major.axis, base = 10), nclass = 50, xlim = c(1, 6), col = &quot;grey75&quot;, axes = FALSE, xlab = &quot;Error elipse semi-major axis length in km (log scale)&quot;, ylab = &quot;count&quot;, main = &quot;&quot;) axis(2, las = 1) axis(1, at = 1:6, lab = 10^(1:6)/1000) hist(log(zloc$Error.Semi.minor.axis, base = 10), nclass = 25, xlim = c(1, 6), col = &quot;grey75&quot;, axes = FALSE, xlab = &quot;Error elipse semi-minor axis length in km (log scale)&quot;, ylab = &quot;count&quot;, main = &quot;&quot;) axis(2, las = 1) axis(1, at = 1:6, lab = 10^(1:6)/1000) Figure 9.2: Error ellipse orientation and error distribution for 2017 Z. cavirostris unfiltered Argos positions. "],["resources.html", "10 Other resources 10.1 Wildlife Computers Documentation 10.2 Argos Documentation 10.3 Goniometer Documentation", " 10 Other resources 10.1 Wildlife Computers Documentation Most of the documentation can be found at https://wildlifecomputers.com/support/downloads/. Many of these files are periodically updated. In particular the tag user guides (SPLASH10 User Guide, SPOT User Guide) are helpful as well as the Spreadsheet File Descriptions. 10.2 Argos Documentation The venerable Argos User’s Manual! 10.3 Goniometer Documentation Installation and user Manual V6 Advice and tips "],["gloss.html", "11 Glossary", " 11 Glossary Transducer A device which converts one type of energy to another. For example, pressure transducers convert pressure into electrical signals. Also called “sensors”. Pin (v.) of a sensor: to get stuck reading one value even as the quantity measured is changing. For example, a pressure sensor may pin at some max value even as pressure increases. Can also pin to low values. Sometimes also “peg”. Argos A system of polar orbiting satellites and ground based stations which can locate beacons using Doppler calculations as well as uplink data. Platform or PTT A particular Argos beacon or transmitter. PTT stands for platform terminal transmitter. LUT Local user terminal. The ground based stations that download data from satellites. Buffer In general a region of memory used for temporary data. We generally use it to refer to the messages queued for transmit. See § 3.2.12. "],["references.html", "References", " References Andrews, R. D., R. L. Pitman, and L. T. Ballance. 2008. Satellite tracking reveals distinct movement patterns for Type B and Type C killer whales in the southern Ross Sea, Antarctica. Polar Biology 31:1461–1468. Baird, R. W., D. L. Webster, G. S. Schorr, D. J. McSweeney, and J. Barlow. 2008. Diel variation in beaked whale diving behavior. Marine Mammal Science 24:630–642. Baird, R. W., D. L. Webster, Z. T. Swaim, H. J. Foley, D. B. Anderson, and A. J. Read. 2017. Spatial Use by Odontocetes Satellite Tagged off Cape Hatteras, North Carolina in 2016. Prepared for U.S. Fleet Forces Command. Submitted to Naval Facilities Engineering Command Atlantic, Norfolk Virginia, under Contract No. N62470-15-D8006, Task Order 28, issued to HDR Inc., Virginia Beach, Virginia. August 2017. Baird, R. W., D. L. Webster, Z. T. Swaim, H. J. Foley, D. B. Anderson, and A. J. Read. 2018. Spatial Use by Cuvier’s Beaked Whales and Short-finned Pilot Whales Satellite Tagged off Cape Hatteras, North Carolina in 2017. Prepared for U.S. Fleet Forces Command. Submitted to Naval Facilities Engineering Command Atlantic, Norfolk, Virginia, under Contract No. N62470-15-D-8006, Task Order 50, Issued to HDR, Inc., Virginia Beach, Virginia. March 2018. Quick, N. J., W. R. Cioffi, J. Shearer, and A. J. Read. 2019. Mind the gap: Optimizing satellite tag settings for time series analysis of foraging dives in Cuvier’s beaked whales (Ziphius cavirostris). Animal Biotelemetry 7:5. Schorr, G. S., E. A. Falcone, D. J. Moretti, and R. D. Andrews. 2014. First long-term behavioral records from Cuvier’s beaked whales (Ziphius cavirostris) reveal record-breaking dives. PloS one 9:e92633. Shearer, J. M. et al. 2019. Diving behaviour of cuvier’s beaked whales (Ziphius cavirostris) off Cape Hatteras, North Carolina. Royal Society Open Science 6:181728. Tyack, P. L., M. Johnson, N. A. Soto, A. Sturlese, and P. T. Madsen. 2006. Extreme diving of beaked whales. Journal of Experimental Biology 209:4238–4253. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
